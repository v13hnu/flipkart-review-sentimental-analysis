{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urlunparse, parse_qs, urlencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From given parsed HTML, returns title, price, ratings, review_title, review, as dictionary and next_path-for next page as string \n",
    "def get_details(soup):\n",
    "    reviews = {}\n",
    "    #Checking whether Title is given for the product\n",
    "    try:\n",
    "        title = soup.find(class_=\"_2s4DIt _1CDdy2\").get_text().strip()\n",
    "        title = title[:-8]\n",
    "        title = remove_comma(title)\n",
    "    except:\n",
    "        title = \"Not Found\"\n",
    "\n",
    "    #Checking whether price detail is given for the product\n",
    "    try:\n",
    "        price = soup.find(class_=\"_30jeq3\").get_text().strip()\n",
    "        price = price[1:]\n",
    "        price = remove_comma(price, replace='')\n",
    "    except:\n",
    "        price = \"Not Found\"\n",
    "    \n",
    "    #checking for reviews, reviews are arranged as columns, which has bundle of information like rating, review and name of person\n",
    "    try:\n",
    "        re_cols = soup.find_all('div', class_='col _2wzgFH K0kLPL')\n",
    "        for i, col in enumerate(re_cols):\n",
    "            reviews[i] = {}\n",
    "            reviews[i]['title'] = title\n",
    "            reviews[i]['price'] = price\n",
    "\n",
    "            try:\n",
    "                reviews[i]['rating'] = col.find(class_='_3LWZlK _1BLPMq').get_text().strip()\n",
    "                reviews[i]['review_title'] = col.find(class_='_2-N8zT').get_text().strip()\n",
    "                review = col.find(class_='t-ZTKy')\n",
    "                review = review.div.div.get_text().strip()\n",
    "                review = review[:-9] #removing 'READ MORE\n",
    "                reviews[i]['review'] = review\n",
    "\n",
    "                reviews[i]['review_title'] = remove_comma(reviews[i]['review_title']) #removing comma to be saved as csv\n",
    "                reviews[i]['review'] = remove_comma(reviews[i]['review'])\n",
    "            except:\n",
    "                reviews[i]['rating'] = -1\n",
    "                reviews[i]['review_title'] = \"Review Not Found\"\n",
    "                reviews[i]['review'] = \"\"\n",
    "    except:\n",
    "        i = 0\n",
    "        reviews[i] = {}\n",
    "        reviews[i]['title'] = title\n",
    "        reviews[i]['price'] = price\n",
    "        reviews[i]['rating'] = -1\n",
    "        reviews[i]['review_title'] = \"Review Not Found\"\n",
    "        reviews[i]['review'] = \"\"\n",
    "    next_path = ''\n",
    "    #checking whether a Next page of review is there and getting the link\n",
    "    try:\n",
    "        #next_link = soup.find('div', class_='_2zg3yZ _3KSYCY')\n",
    "        next_link = soup.find_all(class_='_1LKTO3')\n",
    "        \n",
    "        for x in next_link:\n",
    "            if x.get_text() == 'Next':\n",
    "                next_path =  x['href']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return reviews, next_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For converting Product url into review page url\n",
    "def convert_url_review(URL):\n",
    "\n",
    "    o = urlparse(URL)\n",
    "    scheme = o.scheme\n",
    "    netloc = o.netloc\n",
    "    path = o.path.split('/')\n",
    "\n",
    "    path[2] = 'product-reviews'\n",
    "\n",
    "    path = '/'.join(path)\n",
    "    query = o.query\n",
    "    query = parse_qs(query)\n",
    "    selected = ['pid','lid','marketplace']\n",
    "    new_query = {}\n",
    "    if 'marketplace' not in query:\n",
    "        query['marketplace'] = 'FLIPKART'\n",
    "    for sel in selected:\n",
    "        new_query[sel] = query[sel]\n",
    "\n",
    "    new_query = urlencode(new_query, doseq=True)\n",
    "    url_tuple =(scheme, netloc, path, '', new_query, '')\n",
    "    URL = urlunparse(url_tuple)\n",
    "\n",
    "    return URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For getting next page of review\n",
    "def get_next_url(URL, next_path):\n",
    "\n",
    "    o = urlparse(URL)\n",
    "    scheme = o.scheme\n",
    "    netloc = o.netloc\n",
    "    path = next_path\n",
    "    url_tuple =(scheme, netloc, path, '', '', '')\n",
    "    URL = urlunparse(url_tuple)\n",
    "\n",
    "    return URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes comma from the text so that there is no error while saving or reading CSV\n",
    "def remove_comma(ori_text, replace=\" \"):\n",
    "    text = ori_text.split(',')\n",
    "    new_text = replace.join(text)\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(filename):\n",
    "\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    #use your user-agent . If you don't know search in Google \"My user agent\"\n",
    "    header =  {\"User-Agent\":'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'}\n",
    "\n",
    "    #file1 -> url list\n",
    "    #file2 -> reviews.txt\n",
    "    #file1 -> reviews.csv\n",
    "\n",
    "    file1 = open('urls.txt', 'r') \n",
    "    urls = file1.readlines() \n",
    "    file1.close()\n",
    "    # file2 = open('reviews.txt','w')\n",
    "    file3 = open('reviews.csv','w')\n",
    "\n",
    "    file3.writelines('Item,Price,Rating,Review\\n')\n",
    "\n",
    "    #iterating through the urls provided in the file\n",
    "    for URL in urls:\n",
    "        #checking whether URL is valid or not\n",
    "        if URL == '\\n':\n",
    "            continue\n",
    "        try:\n",
    "            page = requests.get(URL, headers=header)\n",
    "        except:\n",
    "            # print(\"Unreachable or Invalid URL\")\n",
    "            continue\n",
    "\n",
    "        URL = convert_url_review(URL)\n",
    "        #for checking whether next page exists\n",
    "        isnext = True\n",
    "        #Tracking the run, run+1 at any stage is equal to number of pages it has been through\n",
    "        run = 0\n",
    "\n",
    "        #going through the next links\n",
    "        while(isnext):\n",
    "            try:\n",
    "                page = requests.get(URL, headers=header)\n",
    "            except:\n",
    "                print(\"Unreachable or Invalid URL\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            reviews, next_path = get_details(soup)\n",
    "            for review in reviews.values():\n",
    "                if review['rating'] != -1 :\n",
    "                    # if run == 0:\n",
    "                        # file2.writelines(review['title']+ '\\n')\n",
    "                        # file2.writelines('Price : ' +review['price'] + '\\n') \n",
    "                    # file2.writelines( \"'\" + review['review_title'] +\"' \" + review['rating'] + \" \" + review['review'] + \"\\n\")\n",
    "                    file3.writelines(review['title']+','+review['price']+','+  review['rating']+\",'\" + review['review_title'] +\"' \" + review['review'] + \"\\n\")\n",
    "            #if needed to limit number of runs uncomment\n",
    "            if ( next_path == ''): # or (run == 100):\n",
    "                isnext = False\n",
    "                continue\n",
    "            else:\n",
    "                URL = get_next_url(URL, next_path)\n",
    "            run += 1\n",
    "        \n",
    "        # print(\"URL \",URL)\n",
    "        # print(\"next_path \",next_path)\n",
    "\n",
    "    # file2.close()\n",
    "    file3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    #Checking whether the filename is passed as an argument\n",
    "    if len(sys.argv) > 1:\n",
    "       main(sys.argv[1])\n",
    "    else:\n",
    "        print(\"usage: python flipkart-reviews.py filename.txt #where filename.txt contains URLs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
